# 浏览器请求的url

- url
 - 在Chrome中右键点击检查，在Network中寻找
 - url = 协议+网站域名+资源路径+参数

- 浏览器请求url地址的时候
 - 当前url对应的响应+`js`+`css` // `elements`中的内容
- 爬虫请求url地址的时候
 - 只有当前url对应的响应

# http和https

- http // 超文本传输协议
 - 以明文形式传输
- https // http + ssl（安全套接字层）
 - 加密——传输——解密

- http协议之请求
 - 在Headers（View Source）
 - 1.请求行 // 在最上面GET那里
 - 2.请求头
  - Host // 域名
  - Connection // 是否支持长链接（1为支持）
  - User-Agent // 用户代理（浏览器身份标识），可以通过改变它来模仿不同浏览器的访问
  - Accept // 接收的数据类型
  - Accept-Encoding // 接收数据的编码格式
  - Accept-Language // 接收数据的语言
  - Cookie // 存储用户信息，每次请求会被携带上发送给对方的浏览器
   - 要获取登陆后才能访问的页面的时候使用
   - 对方服务器通过Cookie来判断是否是爬虫
 - 3.请求体 // 携带一些数据
  - Get请求没有请求体（数据放到url地址里面），Post请求有请求体
   - Post请求常用于登陆注册，或传输大文本（因为携带数据量大）

- http协议之相应
 - 响应头
  - Set Cookie // 对方服务器通过该字段设置Cookie到本地
 - 相应体（Response）
  - url地址对应的响应

# request模块

首先要安装 `pip install requests`

## 使用get，post获取响应

```
response = requests.get(url) #发送get请求
response = requests.get(url, data = {请求体的字典})
```
得到的是响应的码

## Response的方法

获取网页的html字符串：
```
print(response.text) # 不加解码格式一般是乱码
```
使用`response.encoding = ''`设置解码格式，会得到一些中文显示（比乱码好一些）
或者`response.content.decode()`，把响应的二进制字节转化为str类型

所以获取网页源码的正确方式：
```
reponse.content.decode() # 第一个最简单的方式
reponse.content.decode("gbk") # 第一个获取不到的化试试第二种 （gbk 中文编码格式）
response.text # 都不行的话用这个
```

一些其他方法：
```
response.request.url # 发送请求的url地址
response.url # 响应的url地址
response.request.headers # 请求头
response.headers # 响应头
```

## 发送带有headers的请求

加上一个字典：
headers = {"请求头冒号前面的内容": "请求头冒号后面的内容"}

然后在后面传递：
```
response = requests.post(url, headers = headers)
```
带上headers之后对方服务器就不会轻易认定为爬虫，可以得到更多数据

## 超时参数

使用超时参数可以防止请求一个url地址时花时间过长，作用是规定这个请求的最长响应时间
```python
response = requests.get("url", headers = headers, timeout = 3)
# timeout = 3 意思是三秒之内必须返回响应，否则会报错
```

## retrying模块

第三方模块，需要安装
```
pip install retrying
```
作用是重复执行某个方法。

需要按装饰的方法使用，例子:
```python
import requests
from retrying import retry

headers = {}

@retry(stop_max_attempt_number = 3) # 被装饰的函数最多执行三次，其中有一次成功则继续执行，三次都错误则报错
def _parse_url(url):
  print('*'*100)
  response = requests.get(url, headers = headers, timeout = 5)
  return response.content.decode()

def parse_url(url):
  try: # 为了不报错，定义两个函数，如果请求失败的话则返回None值
    html_str = _parse_url(url)
  except:
    html_str = None
  return html_str

if __name__ == __main__:
  url = "http://www.baidu.com"
  print(parse_url(url))
```

## Cookie相关的请求

- 直接携带Cookie
 - 将Cookie内容添加到headers字典中
  - tips：将获取的网页字符串写到本地的时候，如果出做需要加上编码格式，例：
```python
with open("test1.html", "w", encoding = 'utf-8') as f:
f.write(response.content.decode())
```
 - 新建一个Cookie字典，作为get方法的参数(tips：一种创建字典快捷的办法)：
```python
cookies = "a=b;  c=d" # 抓包得来的cookies
# cookies 字符串一般很长，自己复制建字典比较麻烦，可以用下面的方法：
cookie_dict = {i.split("=")[0]:i.split("=")[-1] for i in cookies.split("; ")}
response = requests.get("url", headers = headers, cookies = cookie_dict)
```
- 发送post请求，先获取cookie，使用session保存，再用session直接请求
 - 使用session：
```python
session = requests.session() # session设为requests的一个实例
url = " " # 这里的url是用户登陆那里的url地址（一般是login字样）
data = { }   # 这里data中是用户名和密码，去网页抓包得到他们的键
session.post(url,data = data, headers = headers) # cookies会存在session中，下次使用session会自动带上cookies
session.get(url) # 这里就已经包含cookies了
```

*——by 秦小炅 2018.9.15*
