在Linux系统中，每个I/O都是文件，对应一个文件描述符`fd`，I/O多路复用机制就是在`单一线程内`对fd的`监听`，当fd有事件发生时进行处理

最简单的方法就是在程序中对每个fd进行轮询，当有事件发生时进行处理，不过这样效率比较低

为了解决这个问题，人们开发了select，poll，epoll三种函数来优化I/O复用

select，poll和epoll本质上都是`同步I/O`，需要自己负责读写，并且会`阻塞`

## 1.select

select函数维护一个长度为`1024`的`bitmap`，每个位映射到一个fd，为1的位表示对应的fd有事件发生，需要进行处理

处理流程为：

- 1.首先`0~最大fd`这一段bitmap（`fd_set`）从用户态复制到`内核态`，因为本身判断fd是否就绪就需要在内核中执行，这样省去了每个fd都要执行用户态到内核态切换到开销

- 2.主函数在select处`阻塞`，等待可以执行的fd（被置1）

- 3.内核中不断对这些位对应的fd进行轮询，当出现就绪的fd时，将对应的位`置1`，将控制权给到相应的处理程序

- 4.在下一次select之前需要重新对fd进行配置

select有几个缺点：

- 1.bitmap只有1024位，数量有限制

- 2.在内核中对bitmap进行轮询复杂度为`O(n)`

- 3.每次需要重置fd，带来额外的开销

## 2.poll

poll通过一个称为pollfd的结构体来表示fd，如下所示：

```
pollfd {
    int fd
    short events
    short revents
}
```

与select不同，poll是基于链表来存储pollfd的，所以`没有连接数量的限制`

在poll函数中，设置fd就绪的方法是将pollfd.revents置1，而且进入下一次循环时，只需要将`pollfd.revents重新置0`，而不需要彻底的重构fd，省去了这一部分的开销

所以poll改进了select的两个缺点：最大连接数的限制与fd的重构

## 3.epoll

epoll通过一个ev结构体来映射fd，如下所示

```
ev {
    int fd
    int events
}
```

在epoll中，使用`红黑树`来存储ev，在内核中进行监听时，不是通过置为操作来设置就绪fd，而是每当发现一个就绪的fd，就将它放到数据结构的`开始处`，这样只需要返回所有`就绪的fd数量n`，而程序就知道`前面的n个fd都是就绪的`，不需要再次遍历，这样的复杂度是`O(1)`
