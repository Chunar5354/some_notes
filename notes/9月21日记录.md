## 保存图片方法

- 1.urllib.request.urlretrieve
 - 方法：`urllib.request.urlretrieve(图片地址, 保存路径)`


## 延时

爬取大量数据时，容易被对方服务器识别为爬虫程序而关闭连接，这时需要一个延迟
- 最简单的：`time.sleep(number)`，最好设置成随机数字，time模块需要导入

- tips:
 - 做爬虫文件时，应该尽可能地多加一些headers，也可以多用几个不同的UsreAgent，来增加不被反爬的概率
 - 最好做一个延时，让每次请求间隔随机的时间
 - 有一些Windows自带的python库树莓派没有


今天除了优化一些爬取街拍图片的文件啥也没干，不要脸的把程序贴到这里，里面有很多知识点都写在注释里啦：

```python
import requests
import json
import re
import urllib
from bs4 import BeautifulSoup
import lxml
import time
import random

class Jiepai():
    def __init__(self):
        self.data =  {"offset": "0",
                    "format": "json",
                    "keyword": "街拍",
                    "autoload": "true",
                    "count": "20",
                    "cur_tab": "1",
                    "from": "search_tab"
                    }
        self.start_url = ("https://www.toutiao.com/search_content/?") # + urllib.parse.urlencode(self.data))
        self.headers =[{"user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36",
                        "cookie": "tt_webid=6602519899502626318; WEATHER_CITY=%E5%8C%97%E4%BA%AC; UM_distinctid=165ec5eb1b27bf-02b9b2848bc364-9393265-144000-165ec5eb1b37ad; tt_webid=6602519899502626318; csrftoken=4630032c0ed8e032160fc3fe163317a3; __tasessionId=7ew4hr7t01537529405505; CNZZDATA1259612802=1468619485-1537267342-https%253A%252F%252Fwww.baidu.com%252F%7C1537526542",
                        "referer": "https://www.toutiao.com/"},
                        {"User-Agent":"Mozilla/5.0(compatible;MSIE9.0;WindowsNT6.1;Trident/5.0",
                        "cookie": "tt_webid=6602519899502626318; WEATHER_CITY=%E5%8C%97%E4%BA%AC; UM_distinctid=165ec5eb1b27bf-02b9b2848bc364-9393265-144000-165ec5eb1b37ad; tt_webid=6602519899502626318; csrftoken=4630032c0ed8e032160fc3fe163317a3; __tasessionId=7ew4hr7t01537529405505; CNZZDATA1259612802=1468619485-1537267342-https%253A%252F%252Fwww.baidu.com%252F%7C1537526542",
                        "referer": "https://www.toutiao.com/"},
                        {"User-Agent":"Mozilla/5.0(WindowsNT6.1;rv:2.0.1)Gecko/20100101Firefox/4.0.1",
                        "cookie": "tt_webid=6602519899502626318; WEATHER_CITY=%E5%8C%97%E4%BA%AC; UM_distinctid=165ec5eb1b27bf-02b9b2848bc364-9393265-144000-165ec5eb1b37ad; tt_webid=6602519899502626318; csrftoken=4630032c0ed8e032160fc3fe163317a3; __tasessionId=7ew4hr7t01537529405505; CNZZDATA1259612802=1468619485-1537267342-https%253A%252F%252Fwww.baidu.com%252F%7C1537526542",
                        "referer": "https://www.toutiao.com/"},
                        {"User-Agent":"Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;360SE",
                        "cookie": "tt_webid=6602519899502626318; WEATHER_CITY=%E5%8C%97%E4%BA%AC; UM_distinctid=165ec5eb1b27bf-02b9b2848bc364-9393265-144000-165ec5eb1b37ad; tt_webid=6602519899502626318; csrftoken=4630032c0ed8e032160fc3fe163317a3; __tasessionId=7ew4hr7t01537529405505; CNZZDATA1259612802=1468619485-1537267342-https%253A%252F%252Fwww.baidu.com%252F%7C1537526542",
                        "referer": "https://www.toutiao.com/"}
                        ]

    def get_index(self, url):
        header_num = random.randint(0,3)
        response = requests.get(url, headers = self.headers[header_num])
        html_str = response.content.decode()
        return html_str

    def parse_index(self, html_str):
        html_dict = json.loads(html_str)
        data_list = html_dict.get('data')
        url_list = []
        for data in data_list:
            the_url = data.get('article_url')
            url_list.append(the_url)
        while None in url_list:
            url_list.remove(None) # 删除列表中的None值，  列表删除操作remove
        return url_list # 到这里得到的是一系列含有组图url地址的list，但是里面含有None，怎么去掉呢？在上面解决啦

    def get_url(self, url):
        header_num = random.randint(0,3)
        response = requests.get(url, headers = self.headers[header_num])
        html_str = response.content.decode()
        html_str = html_str.replace('\\', '')
        return html_str

    def parse1_url(self, html_str): # 第一种解析方法
        pattern = re.compile('&lt;img src&#x3D;&quot;(.*?)&quot; img_widt', re.S) # 强行找到url，费眼睛
        result = re.findall(pattern, html_str) # 得到包含图片url地址的字符串,返回的是一个list
        # result = json.loads(result)
        return result

    def parse2_url(self, html_str): # 第二种解析方法
        pattern = re.compile('rl_list":\[(.*?)],"uri', re.S) # 强行找到url，费眼睛
        result = re.findall(pattern, html_str) # 得到包含图片url地址的字符串,返回的是一个list
        # result = json.loads(result)
        return result

    def get_url_list(self, url_list):
        the_url_list = []
        for url_elem in url_list:

            html = self.get_url(url_elem)
            if html:
                # print(html)
                result1 = self.parse1_url(html)
                result2 = self.parse2_url(html)
                #print(result)

                for i in result1:
                    # the_url_list.append(i)
                    if i:
                        the_url_list.append(i)
                        # print("获取成功")

                for j in result2:
                    if j:
                        j_parse = re.search('url":"(.*?)"', j)
                        # print(type(j_parse))
                        the_j = j_parse.group(1)
                        the_url_list.append(the_j)
                        # print("获取成功")
        return the_url_list

    def save_img(self, the_url_list, page_num):
        save_num = 1
        for img_url in the_url_list:
            urllib.request.urlretrieve(img_url, 'E:/atomfile/photo/{}{}.jpg'.format(page_num, save_num))
            time_num = random.uniform(0,2)
            num_ = round(time_num, 2)
            time.sleep(num_)
            print("保存成功{}".format(save_num))
            save_num += 1

    def run(self):
        # 1.索引页url响应获取
        data = self.data
        num = 0
        page_num = 1
        while num < 101:

            data['offset'] = num
            url = self.start_url + urllib.parse.urlencode(data)
            html_str = self.get_index(url)
            # print(type(html_str))
            # 2.解析索引页html字符串，得到各组图的url地址
            url_list = self.parse_index(html_str)
            # print(url_list[0])

            # 3.组图url响应获取
            the_url_list = self.get_url_list(url_list)
            page = 'page{}_'.format(page_num)
            self.save_img(the_url_list, page)
            num += 20
            page_num += 1

            # print(the_url_list)
            # 4.保存图片到本地


jiepai = Jiepai()
jiepai.run()
```

